# ğŸ  Projeto: AnÃ¡lise do Mercado ImobiliÃ¡rio com Arquitetura de MedalhÃ£o

Este repositÃ³rio apresenta um projeto completo de **engenharia e anÃ¡lise de dados** aplicado ao mercado imobiliÃ¡rio, utilizando uma base pÃºblica do Kaggle.  
O projeto cobre desde a ingestÃ£o de dados brutos atÃ© a **modelagem dimensional (Data Warehouse)** e a criaÃ§Ã£o de **consultas analÃ­ticas avanÃ§adas**, seguindo uma **arquitetura de dados em medalhÃ£o (Raw â†’ Silver â†’ Gold)**.

ğŸ”— **Base de dados utilizada:**  
https://www.kaggle.com/datasets/shengkunwang/housets-dataset

ğŸ”— **Dashboard Power BI (publicado):**  
https://app.powerbi.com/links/kBHH3nr9-3?ctid=ec359ba1-630b-4d2b-b833-c8e6d48f8059&pbi_source=linkShare

---

## Objetivos do Projeto

- Implementar um pipeline de dados estruturado seguindo boas prÃ¡ticas de engenharia de dados  
- Aplicar a arquitetura de medalhÃ£o em um contexto analÃ­tico real  
- Realizar anÃ¡lises exploratÃ³rias e analÃ­ticas do mercado imobiliÃ¡rio  
- Avaliar impactos de fatores socioeconÃ´micos, infraestrutura e tempo nos preÃ§os dos imÃ³veis  
- Construir um **Data Warehouse** com modelo estrela  
- Preparar dados e consultas para visualizaÃ§Ã£o em ferramentas de BI (Power BI)  

---

## Estrutura de Pastas do Projeto
```
.
â”œâ”€â”€ data_layer
â”‚ â”œâ”€â”€ raw
â”‚ â”‚ â”œâ”€â”€ analytics
â”‚ â”‚ â”œâ”€â”€ dados_brutos.csv
â”‚ â”‚ â””â”€â”€ dicionario_de_dados
â”‚ â”‚
â”‚ â”œâ”€â”€ silver
â”‚ â”‚ â”œâ”€â”€ analytics
â”‚ â”‚ â”œâ”€â”€ mer_der_dld
â”‚ â”‚ â””â”€â”€ ddl
â”‚ â”‚
â”‚ â””â”€â”€ gold
â”‚ â”œâ”€â”€ consultas.sql
â”‚ â”œâ”€â”€ ddl
â”‚ â”œâ”€â”€ mnemonico
â”‚ â””â”€â”€ mer_der_dld
â”‚
â”œâ”€â”€ transformer
â”‚ â”œâ”€â”€ raw_to_silver.py
â”‚ â””â”€â”€ silver_to_gold.py
â”‚
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```


---

## Arquitetura de Dados (MedalhÃ£o)

O projeto segue uma **arquitetura de medalhÃ£o adaptada**, adequada para projetos analÃ­ticos de pequeno e mÃ©dio porte.

### Raw (Bronze)
- Dados brutos carregados diretamente do arquivo CSV  
- Nenhuma transformaÃ§Ã£o aplicada  
- Preserva o dado original para auditoria e rastreabilidade  
- ContÃ©m anÃ¡lises exploratÃ³rias iniciais (EDA)  

---

### Silver
- Dados limpos, padronizados e tipados  
- PadronizaÃ§Ã£o de nomes de colunas  
- ConversÃ£o de tipos de dados  
- Limpeza de textos e valores invÃ¡lidos  
- CriaÃ§Ã£o de variÃ¡veis derivadas (`month`, `season`)  
- PreparaÃ§Ã£o para anÃ¡lises exploratÃ³rias consolidadas  
- Armazenamento em PostgreSQL no schema `silver`  

---

### Gold
- Modelagem dimensional (Data Warehouse)  
- SeparaÃ§Ã£o em **tabelas dimensÃ£o** e **tabela fato**  
- Uso de **chaves substitutas (SRK)**  
- PadronizaÃ§Ã£o por **mnemÃ´nicos**  
- Consultas analÃ­ticas avanÃ§adas  
- Estrutura otimizada para BI e anÃ¡lises estratÃ©gicas  

---

## Pipeline ETL

### Raw â†’ Silver

Script responsÃ¡vel:

transformer/etl_raw_to_silver.ipynb


**Etapas executadas:**
1. ConexÃ£o com PostgreSQL  
2. CriaÃ§Ã£o automÃ¡tica do schema `silver`  
3. Leitura dos dados brutos (`dados_brutos.csv`)  
4. PadronizaÃ§Ã£o dos nomes das colunas  
5. ConversÃ£o de tipos (datas e numÃ©ricos)  
6. Limpeza de dados textuais  
7. CriaÃ§Ã£o de colunas derivadas  
8. Carga da tabela `silver.silver_houses`  

---

### Silver â†’ Gold

Script responsÃ¡vel:

transformer/etl_silver_to_gold.ipynb


**Etapas executadas:**
1. Leitura dos dados da camada silver  
2. AplicaÃ§Ã£o dos mnemÃ´nicos definidos  
3. CriaÃ§Ã£o das tabelas dimensÃ£o  
4. GeraÃ§Ã£o das chaves substitutas (SRK)  
5. ConstruÃ§Ã£o da tabela fato  
6. CriaÃ§Ã£o de PKs, FKs e Ã­ndices  
7. Carga no schema `dw`  

---

## Modelagem Dimensional

O Data Warehouse segue o **modelo estrela**.

### DimensÃµes
- `dim_tmp`  
- `dim_lcl`  
- `dim_soc`  
- `dim_inf`  

Cada dimensÃ£o possui:
- Chave primÃ¡ria substituta (**SRK**)  
- Atributos descritivos padronizados  

### Tabela Fato
- `fat_hou`  
- MÃ©tricas do mercado imobiliÃ¡rio  
- Relacionamento com todas as dimensÃµes  

Diagramas **MER, DER e DLD** estÃ£o disponÃ­veis nas pastas das camadas **silver** e **gold**.

---

## AnÃ¡lises ExploratÃ³rias e AnalÃ­ticas

### AnÃ¡lises ExploratÃ³rias (EDA)
- DistribuiÃ§Ã£o dos preÃ§os dos imÃ³veis  
- DistribuiÃ§Ã£o da renda per capita  
- RelaÃ§Ã£o renda Ã— preÃ§o dos imÃ³veis  
- AnÃ¡lise de liquidez do mercado  
- AnÃ¡lises temporais e sazonais  

### AnÃ¡lises AnalÃ­ticas (Gold)
- Ranking de cidades mais caras e mais baratas  
- AnÃ¡lise de custo-benefÃ­cio de infraestrutura  
- Ãndices normalizados (0â€“100)  
- PressÃ£o do mercado imobiliÃ¡rio  
- AnÃ¡lises socioeconÃ´micas  
- Consultas avanÃ§adas utilizando CTE  

---

## Dashboards e AnÃ¡lises (Power BI)

Os dashboards foram organizados para contar uma **histÃ³ria analÃ­tica progressiva**:

### Panorama do Mercado
- PreÃ§o mÃ©dio dos imÃ³veis  
- Volume de vendas  
- Percentual de imÃ³veis vendidos acima do preÃ§o  
- Cidades mais caras e mais acessÃ­veis  
- PressÃ£o do mercado imobiliÃ¡rio  

### Infraestrutura e Perfil EconÃ´mico
- Ãndice de custo-benefÃ­cio urbano  
- Renda mÃ©dia por cidade  
- Infraestrutura urbana  
- Ãndice de eficiÃªncia urbana  

### DinÃ¢mica Temporal
- EvoluÃ§Ã£o dos preÃ§os ao longo do tempo  
- EvoluÃ§Ã£o da infraestrutura  
- AnÃ¡lise de sazonalidade (Nova Iorque)  

Essas anÃ¡lises permitem compreender nÃ£o apenas *quanto* custam os imÃ³veis, mas *por que* eles custam esse valor e *como* o mercado se comporta ao longo do tempo.

---

## Tecnologias Utilizadas

- **Linguagem:** Python  
- **Banco de Dados:** PostgreSQL  
- **ETL / AnÃ¡lise:** Pandas, NumPy  
- **VisualizaÃ§Ã£o:** Power BI  
- **Ambiente:** Jupyter Notebook  
- **ContainerizaÃ§Ã£o:** Docker / Docker Compose  


---

## ConclusÃ£o

Este projeto demonstra a aplicaÃ§Ã£o prÃ¡tica de conceitos de **engenharia de dados**, **Data Warehouse** e **Business Intelligence**, transformando dados pÃºblicos em insights estratÃ©gicos sobre o mercado imobiliÃ¡rio dos Estados Unidos.

A abordagem adotada permite anÃ¡lises robustas, escalÃ¡veis e facilmente integrÃ¡veis a ferramentas de visualizaÃ§Ã£o, oferecendo suporte qualificado Ã  tomada de decisÃ£o.

---

## Como rodar o projeto?

Inicie o banco de dados PostgreSQL utilizando Docker Compose:

```bash
docker compose up -d
```
---

## Instale as dependÃªncias

pip install -r requirements.txt

---
## Ordem dos procedimentos

Execute os notebooks na ordem abaixo.

1. data_layer/raw/analytics.ipynb

2. transformer/etl_raw_to_silver.ipynb

3. data_layer/silver/analytics.ipynb

4. transformer/etl_silver_to_gold.ipynb
